In my preliminary exploration, I have observed 69 out of 103 (66.7%) supervised machine learning based repositories to not include test cases at all, which motivates me to study if supervised machine learning based repositories have test cases that check for adversarial attacks. 

I did a scoping review of the flagship software engineering venues, namely ICSE, FSE, ASE, ISSTA, ICST, and observe label perturbation attack to be under-explored. The threat model to conduct a label perturbation attack is described as follows:

#### Threat Model: 
In our threat model, we assume that the adversaries can access the training data along with the ground truth of the training data, but cannot access the training process. For example, the adversaries can be those who have access to the training data storage or the providers of the training data. We also assume the adversaries know nothing of the model under attack. Besides, we assume the detector has access to the set of benign examples but knows nothing about how the adversary generates adversarial examples. To implement the attack, the adversary perturbs the labels for a fraction of the training data to reduce the prediction accuracy of supervised learning systems. The model would behave as expected on most inputs, but inputs modified by the adversary can be mislabeled, which is different from the actual ground truth. 

After the label is perturbed, it is shipped to the developer. The developer, as the victim, will use the modified training data to train the model. During the training of the supervised model, the model will learn the wrong input-output relations. After the training and verification, the model will be deployed into the production environment. The model would behave as expected on most inputs, but inputs modified by the adversary can be mislabeled as something different from the actual ground truth.

#### Proposed Study:
I am proposing an auto test case generation technique that will generate test cases for Label Perturbation attack. I am planning the following tasks: 
(i) test generation via machine translation, and 
(ii) adversarial sample generation with constraint solving.

#### Limitation of Existing Techniques:
- Existing testing techniques such as, Fuzz testing [1], DeepXplore [2], TensorFuzz [3], Deeptest [4], Deepgauage [5], DeepMutation [6] focus on the quality of DL models and effective in detecting exceptions such as crashes, inconsistencies, nan and inf bugs, but they do not focus on generating test cases for adversarial samples.
- [7] proposed a feature-guided test generation that transformed the problem of finding adversarial examples into a two-player turn-based stochastic game. However, their threat model assumed that the attacker has access to the compiled DNN model, which is not valid for the label perturbation attack.
- [8], [3] generates test cases based on fuzzing, which generates random data as inputs. Fuzzing techniques cannot be used to generate test cases for label perturbation attack becasue in label perturbation attack only label of the training data is changed not the actual training data.
- [9], [10], [14], [15] generates test cases based on mutation where a test suite is evaluated on a set of systematically generated artificial faults (mutants). Any surviving mutant that is not detected by the test suite constitutes a concrete test goal, pointing out possible ways to improve the test suite. Mutation techniques cannot be used to generate test cases for label perturbation attack becasue mutation is generated by modifying the actual model, but in our threat model adversaries do not have access to the model.
- [11] generates test cases based on gunit testing, which is an executable piece of code that validates a functionality of a class or a method under test performing as designed. Unit test techniques cannot be used to generate test cases for label perturbation attack becasue it also needs access to the model.
- DeepXplore [2] is a white-box differential test generation technique that uses domain specific constraints on inputs. This technique requires multiple DNN models trained on the same dataset as cross referencing oracles. DeepXplore cannot be used to generate test cases for label perturbation attack becasue in label perturbation attack adversaries do not have access to the model.
- [12] used existing gradient ascent based test generation technique, which also needs access to the model.
- [13] used a novel technique called Surprise Adequacy for Deep Learning Systems (SADL), which is based on the behaviour of DL systems with respect to their training data. They measure the surprise of an input as the difference in DL system’s behaviour between the input and the training data, and subsequently develop this as an adequacy criterion: a good test input should be sufficiently but not overtly surprising compared to training data. Since it also need access to the model, SADL cannot be used to generate test cases for label perturbation attack. 

#### References:
1. J. M. Zhang, M. Harman, L. Ma, and Y. Liu, “Machine learning test- ing: Survey, landscapes and horizons,” IEEE Transactions on Software Engineering, 2020.
2. K. Pei, Y. Cao, J. Yang, and S. Jana, “Deepxplore: Automated whitebox testing of deep learning systems,” in Proceedings of the 26th Symposium
on Operating Systems Principles, 2017, pp. 1–18.
3. A. Odena and I. Goodfellow, “Tensorfuzz: Debugging neural networks with coverage-guided fuzzing,” arXiv preprint arXiv:1807.10875, 2018.
4. Y. Tian, K. Pei, S. Jana, and B. Ray, “Deeptest: Automated testing of deep-neural-network-driven autonomous cars,” in Proceedings of the 40th International Conference on Software Engineering, 2018, pp. 303– 314.
5. L. Ma, F. Juefei, Xu, F. Zhang, J. Sun, M. Xue, B. Li, C. Chen, T. Su, L. Li, Y. Liu et al., “Deepgauge: Multi-granularity testing criteria for deep learning systems,” in Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering, 2018, pp. 120–131.
6.  L. Ma, F. Zhang, J. Sun, M. Xue, B. Li, F. Juefei, Xu, C. Xie, L. Li, Y. Liu, J. Zhao et al., “Deepmutation: Mutation testing of deep learning systems,” in IEEE 29th International Symposium on Software Reliability Engineering. IEEE, 2018, pp. 100–111.
7. M. Wicker, X. Huang, and M. Kwiatkowska, “Feature-guided black-box safety testing of deep neural networks,” in International Conference on Tools and Algorithms for the Construction and Analysis of Systems. Springer, 2018, pp. 408–426.
8. Luo, Weisi, Dong Chai, Xiaoyue Run, Jiang Wang, Chunrong Fang, and Zhenyu Chen. "Graph-based Fuzz Testing for Deep Learning Inference Engines." In 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE), pp. 288-299. IEEE, 2021.
9. Petrović, Goran, Marko Ivanković, Gordon Fraser, and René Just. "Does mutation testing improve testing practices?." In 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE), pp. 910-921. IEEE, 2021.
10. Wang, Zan, Hanmo You, Junjie Chen, Yingyi Zhang, Xuyuan Dong, and Wenbin Zhang. "Prioritizing Test Inputs for Deep Neural Networks via Mutation Analysis." In 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE), pp. 397-409. IEEE, 2021.
11. Wang, Song, Nishtha Shrestha, Abarna Kucheri Subburaman, Junjie Wang, Moshi Wei, and Nachiappan Nagappan. "Automatic Unit Test Generation for Machine Learning Libraries: How Far Are We?." In 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE), pp. 1548-1560. IEEE, 2021.
12. Dola, Swaroopa, Matthew B. Dwyer, and Mary Lou Soffa. "Distribution-aware testing of neural networks using generative models." In 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE), pp. 226-237. IEEE, 2021.
13. Kim, Jinhan, Robert Feldt, and Shin Yoo. "Guiding deep learning system testing using surprise adequacy." In 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE), pp. 1039-1049. IEEE, 2019.
14. Guo, Qianyu, Xiaofei Xie, Yi Li, Xiaoyu Zhang, Yang Liu, Xiaohong Li, and Chao Shen. "Audee: Automated testing for deep learning frameworks." In 2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE), pp. 486-498. IEEE, 2020.
15. Pour, Maryam Vahdat, Zhuo Li, Lei Ma, and Hadi Hemmati. "A Search-Based Testing Framework for Deep Neural Networks of Source Code Embedding." In 2021 14th IEEE Conference on Software Testing, Verification and Validation (ICST), pp. 36-46. IEEE, 2021.
